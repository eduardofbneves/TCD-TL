{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte B. \n",
    "### Elaboração de um conjunto de scripts e funções em Python, NumPy, SciPy e Scikit-learn para realizar as tarefas de Aprendizagem Computacional e Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from ReliefF import ReliefF #https://pypi.org/project/ReliefF/\n",
    "\n",
    "# importar livraria que contem as funcoes das features\n",
    "import featuresFunctions\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ex 4\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# 0. Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(target):\n",
    "    return np.unique(target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Loading human activities dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_human_activities_dataset():\n",
    "    loc = []\n",
    "\n",
    "    for i in range (0,15):\n",
    "        loc.append(\"FORTH_TRACE_DATASET-master/part\" + str(i) + \"/part\" + str(i) + \"dev2.csv\")\n",
    "\n",
    "    array = []\n",
    "    for file in loc:\n",
    "        df = pd.read_csv(file, sep=',', header=None)\n",
    "        array.append(df.to_numpy())\n",
    "\n",
    "    array = np.concatenate(array)\n",
    "    #print(array.shape)\n",
    "\n",
    "    return array\n",
    "\n",
    "# hacti - HumanACTIvities\n",
    "hacti_data = load_human_activities_dataset()\n",
    "hacti_input = hacti_data[:, :11]\n",
    "hacti_target = hacti_data[:, -1]\n",
    "\n",
    "hacti_classes = get_classes(hacti_target)\n",
    "\n",
    "dummy = hacti_data[:6]\n",
    "dummy_input = dummy[:, :11]\n",
    "dummy_target = dummy[:, -1]\n",
    "\n",
    "hacti_classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Loading Iris dataset\n",
    "\n",
    "150 instances <br>\n",
    "4 atributes:\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "\n",
    "3 classes:\n",
    "- Iris Setosa\n",
    "- Iris Versicolour\n",
    "- Iris Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "\n",
    "df = pd.DataFrame(iris_data.data, columns = iris_data.feature_names)\n",
    "df['target'] = iris_data.target\n",
    "print(df)\n",
    "iris_df = df\n",
    "iris_input = iris_data.data\n",
    "iris_target = iris_data.target\n",
    "\n",
    "iris_classes = get_classes(iris_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# 1. Data splitting e métricas de exactidão em machine learning\n",
    "### 1.1. Usando o scikit-learn, desenvolva um conjunto de funções para **data splitting** usando dois cenários:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Train-Test (TT) e Train-Validation-Test data split\n",
    "\n",
    "Divides x (input) and y (target) in: <br>\n",
    "- x_train = x * test_perc   \n",
    "- x_val = x * val_perc         \n",
    "- x_test = x - x_train - x_val\n",
    "- y_train = y * test_perc     \n",
    "- y_val = y * val_perc\n",
    "- y_test = y - y_train - y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tt(x, y, test_perc):\n",
    "    test_size = test_perc * 0.01\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=1)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def tvt(x, y, val_perc, test_perc):\n",
    "    test_size = test_perc * 0.01\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=1)\n",
    "\n",
    "    # 80 train, 20 test -> 60 train, 20 validation, 20 test\n",
    "    val_size = test_perc * 0.01\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_size, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. K-fold data split\n",
    "\n",
    "K-folds cross-validator. Returns k indexes of training and validating (min - 2. max - samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_indexes(data_input, splits):\n",
    "    kf = KFold(n_splits=splits)\n",
    "\n",
    "    x_indexes_train=[]\n",
    "    x_indexes_test=[]\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(data_input)):\n",
    "        x_indexes_train.append(train_index)\n",
    "        x_indexes_test.append(test_index)\n",
    "\n",
    "        #print(train_index, test_index)\n",
    "\n",
    "    return x_indexes_train, x_indexes_test\n",
    "    \n",
    "\n",
    "def k_fold_arrays(data_input, data_target, splits):\n",
    "    input_train = []\n",
    "    input_test = []\n",
    "    target_train = []\n",
    "    target_test = []\n",
    "\n",
    "    train_indexes, test_indexes = k_fold_indexes(data_input, splits)\n",
    "    \n",
    "    # new train set according to indexes\n",
    "    for i in train_indexes:\n",
    "        input_train.append(data_input[i])\n",
    "        target_train.append(data_target[i])\n",
    "\n",
    "    # new test set according to indexes\n",
    "    for j in test_indexes:\n",
    "        input_test.append(data_input[j])\n",
    "        target_test.append(data_target[j])\n",
    "\n",
    "    # vstack to turn list of numpy arrays in simple numpy arrays\n",
    "    #input_train = np.vstack(input_train)\n",
    "    #input_test = np.vstack(input_test)\n",
    "    # Input test set:  [array([[1, 2],\n",
    "    #                 [3, 4]]), array([[5, 6]]), array([[7, 8]])]\n",
    "    # Input test set:  [[1 2]\n",
    "    #                 [3 4]\n",
    "    #                 [5 6]\n",
    "    #                 [7 8]]\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"--- ORIGINAL SET: \", data_input)\n",
    "    print(\"Train indexes: \", train_indexes)\n",
    "    print(\"Test indexes: \", test_indexes)\n",
    "    print(\"--- TRAIN SET\")\n",
    "    print(\"Input train set: \", input_train)\n",
    "    print(\"Target train set: \", target_train)\n",
    "    print(\"---TEST SET\")\n",
    "    print(\"Input test set: \", input_test)\n",
    "    print(\"Target test set: \", target_test)\n",
    "    \"\"\"\n",
    "\n",
    "    return input_train, input_test, target_train, target_test\n",
    "\n",
    "### TEST CODE\n",
    "# K-FOLD\n",
    "\n",
    "# dummy = human_activities_data[:6]\n",
    "# dummy_input = dummy[:, :11]\n",
    "# dummy_target = dummy[:, -1]\n",
    "\n",
    "\n",
    "#dummy_train_indexes, dummy_test_indexes = k_fold_indexes(dummy_target, 5)\n",
    "#print(\"---\")\n",
    "\n",
    "\"\"\"\n",
    "x = np.array([[1,2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 10, 20])\n",
    "\n",
    "kf_input_train, kf_input_test, kf_target_train, kf_target_test = k_fold_arrays(x, y, 3)\n",
    "\n",
    "mean_score = 0.00\n",
    "for i in range(len(kf_target_test)):\n",
    "    input_train = kf_input_train[i]\n",
    "    input_test = kf_input_test[i]\n",
    "    target_train = kf_target_train[i]\n",
    "    target_test = kf_target_test[i]\n",
    "\n",
    "print(input_train)\n",
    "print(input_test)\n",
    "print(target_train)\n",
    "print(target_test)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#recall, precision, f1 = exact_methods(dummy_real, dummy_pred, 'macro')\n",
    "#print(recall, precision, f1) # com average = 'micro' os resultados sao iguais\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Usando o scikit-learn, desenvolva um conjunto de funções para cálculo de **métricas de exactidação**, nomeadamente as seguintes:\n",
    "\n",
    "### 1.1.2. Matriz de confusão\n",
    "### 1.2.2. Recall\n",
    "### 1.2.3. Precision\n",
    "### 1.2.4. F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "#from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "\"\"\"\n",
    "def confusion(knn, target_test, target_pred):\n",
    "    cm = confusion_matrix(target_test, target_pred)\n",
    "    matrix = plot_confusion_matrix(knn, cm, target_test, cmap=plt.cm.Blues)\n",
    "    matrix.ax_.set_title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.gcf().axes[0]\n",
    "    plt.gcf().axes[1]\n",
    "    return cm, plt\n",
    "\n",
    "def plot_cm2222(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "    clf = SVC(random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    SVC(random_state=0)\n",
    "    predictions = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
    "    disp.plot()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def plot_cm(target_test, target_pred, classes):\n",
    "    cm = confusion_matrix(target_test, target_pred, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def exact_methods(y_true, y_pred, av):\n",
    "    '''\n",
    "    av = micro\n",
    "    av = macro\n",
    "    av = binary\n",
    "    av = samples\n",
    "    av = weighted\n",
    "    '''\n",
    "    recall = recall_score(y_true, y_pred, average=av) # average=?\n",
    "    precision = precision_score(y_true, y_pred, average=av)\n",
    "    f1 = f1_score(y_true, y_pred, average=av)\n",
    "\n",
    "    return recall, precision, f1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiências iniciais com um classificador simples, i.e., k-Nearest Neighbours (kNN)\n",
    "\n",
    "### 2.1. Usando:\n",
    "- as funções anteriores\n",
    "- o algoritmo kNN\n",
    "- o dataset Iris\n",
    "\n",
    "avalie a capacidade de previsão do algoritmo neste dataset, com **k = 1** e restantes parâmetros por omissão (e.g., métrica de distância, etc.), e usando todo o conjunto de features, nos seguintes cenários:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Train-only, TT 70-30 e 10x10-fold cross-validation (10CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN-ONLY\n",
    "print(\"----- Train-only -----\")\n",
    "\n",
    "target_classes = iris_classes\n",
    "input_train, target_train = iris_input, iris_target\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(input_train, target_train)\n",
    "target_pred = knn.predict(input_train)\n",
    "\n",
    "recall, precision, f1 = exact_methods(target_train, target_pred, 'weighted')\n",
    "cm = plot_cm(target_train, target_pred, target_classes)\n",
    "\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"F1 score: \", f1, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "### TT 70-30\n",
    "print(\"----- TT 70-30 -----\")\n",
    "\n",
    "target_classes = iris_classes\n",
    "input_train, input_test, target_train, target_test = tt(iris_input, iris_target, 30)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(input_train, target_train)\n",
    "target_pred = knn.predict(input_test)\n",
    "\n",
    "recall, precision, f1 = exact_methods(target_test, target_pred, 'weighted')\n",
    "cm = plot_cm(target_test, target_pred, target_classes)\n",
    "\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"F1 score: \", f1, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "### 10-FOLD CV\n",
    "print(\"----- 10-FOLD CV -----\")\n",
    "\n",
    "target_classes = iris_classes\n",
    "fold = 10\n",
    "kf_input_train, kf_input_test, kf_target_train, kf_target_test = k_fold_arrays(iris_input, iris_target, fold)\n",
    "\n",
    "mean_recall = 0.00\n",
    "mean_precision = 0.00\n",
    "mean_f1 = 0.00\n",
    "total_target_test = []\n",
    "total_target_pred = []\n",
    "for i in range(len(kf_target_test)):\n",
    "    input_train = kf_input_train[i]\n",
    "    input_test = kf_input_test[i]\n",
    "    target_train = kf_target_train[i]\n",
    "    target_test = kf_target_test[i]\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(input_train, target_train)\n",
    "    target_pred = knn.predict(input_test)\n",
    "    total_target_test.extend(target_test)\n",
    "    total_target_pred.extend(target_pred)\n",
    "\n",
    "    recall, precision, f1 = exact_methods(target_test, target_pred, 'weighted')\n",
    "    mean_recall += recall\n",
    "    mean_precision += precision\n",
    "    mean_f1 += f1\n",
    "\n",
    "\n",
    "print(\"MEAN RECALL:\", mean_recall/fold)\n",
    "print(\"MEAN PRECISION:\", mean_precision/fold)\n",
    "print(\"MEAN F1 SCORE:\", mean_f1/fold)\n",
    "cm = plot_cm(total_target_test, total_target_pred, target_classes)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Train-only, TVT 40-30-30 e 10x10CV, fazendo variar k na gama {1, 3, 5, …, 15}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TVT 40-30-30\n",
    "print(\"----- TVT 40-30-30 -----\")\n",
    "\n",
    "target_classes = iris_classes\n",
    "input_train, input_val, input_test, target_train, target_val, target_test = tvt(iris_input, iris_target, 57.1428, 30) # 3 simples -> 70-100; 40-x; x = 40*100/70 = 57.1428 \n",
    "\n",
    "k_neigh = [1, 3, 5, 7, 9, 11, 13, 15]\n",
    "\n",
    "mean_recall = 0.00\n",
    "mean_precision = 0.00\n",
    "mean_f1 = 0.00\n",
    "\n",
    "print(\"-- 1st iteration (with val set)\")\n",
    "for k in k_neigh:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(input_train, target_train)\n",
    "    target_val_pred = knn.predict(input_val)\n",
    "\n",
    "    recall, precision, f1 = exact_methods(target_val, target_val_pred, 'weighted')\n",
    "    #cm = plot_cm(target_val, target_val_pred, target_classes)\n",
    "\n",
    "    mean_recall += recall\n",
    "    mean_precision += precision\n",
    "    mean_f1 += f1\n",
    "\n",
    "print(\"MEAN RECALL:\", mean_recall/len(k_neigh))\n",
    "print(\"MEAN PRECISION:\", mean_precision/len(k_neigh))\n",
    "print(\"MEAN F1 SCORE:\", mean_f1/len(k_neigh), \"\\n\")\n",
    "\n",
    "\n",
    "### 10-FOLD CV\n",
    "print(\"----- 10-FOLD CV -----\")\n",
    "\n",
    "target_classes = iris_classes\n",
    "fold = 10\n",
    "kf_input_train, kf_input_test, kf_target_train, kf_target_test = k_fold_arrays(iris_input, iris_target, fold)\n",
    "\n",
    "\n",
    "absolute_mean_recall = 0.00\n",
    "absolute_mean_precision = 0.00\n",
    "absolute_mean_f1 = 0.00\n",
    "\n",
    "absolute_total_target_test = []\n",
    "absolute_total_target_pred = []\n",
    "\n",
    "for k in k_neigh:\n",
    "    mean_recall = 0.00\n",
    "    mean_precision = 0.00\n",
    "    mean_f1 = 0.00\n",
    "\n",
    "    print(\"-- k = \", k)\n",
    "    total_target_test = []\n",
    "    total_target_pred = []\n",
    "    for i in range(len(kf_target_test)):\n",
    "        input_train = kf_input_train[i]\n",
    "        input_test = kf_input_test[i]\n",
    "        target_train = kf_target_train[i]\n",
    "        target_test = kf_target_test[i]\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(input_train, target_train)\n",
    "        target_pred = knn.predict(input_test)\n",
    "        total_target_test.extend(target_test)\n",
    "        total_target_pred.extend(target_pred)\n",
    "\n",
    "        recall, precision, f1 = exact_methods(target_test, target_pred, 'weighted')\n",
    "        mean_recall += recall\n",
    "        mean_precision += precision\n",
    "        mean_f1 += f1\n",
    "\n",
    "\n",
    "    print(\"MEAN RECALL:\", mean_recall/fold)\n",
    "    print(\"MEAN PRECISION:\", mean_precision/fold)\n",
    "    print(\"MEAN F1 SCORE:\", mean_f1/fold)\n",
    "\n",
    "    absolute_mean_recall += mean_recall/fold\n",
    "    absolute_mean_precision += mean_precision/fold\n",
    "    absolute_mean_f1 += mean_f1/fold\n",
    "    absolute_total_target_test.extend(total_target_test)\n",
    "    absolute_total_target_pred.extend(total_target_pred)\n",
    "\n",
    "\n",
    "print(\"ABSOLUTE MEAN RECALL:\", absolute_mean_recall/len(k_neigh))\n",
    "print(\"ABSOLUTE MEAN PRECISION:\", absolute_mean_precision/len(k_neigh))\n",
    "print(\"ABSOLUTE MEAN F1 SCORE:\", absolute_mean_f1/len(k_neigh))\n",
    "cm = plot_cm(absolute_total_target_test, absolute_total_target_pred, target_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = tt(X, y, 0.5)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train) #train\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "acc = knn.score(X_test, y_test) #test\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn_cv = KNeighborsClassifier(n_neighbors=6)\n",
    "#train model with cv of 5 \n",
    "cv_scores = cross_val_score(knn_cv, X, y, cv=10)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print('cv_scores mean:{}'.format(np.mean(cv_scores)))\n",
    "\n",
    "neighbours = np.linspace(1, 15, 8)\n",
    "\n",
    "for n in neighbours:\n",
    "    knn = KNeighborsClassifier(n_neighbors=int(n))\n",
    "    knn.fit(X, y)\n",
    "\n",
    "neigh = np.linspace(1, 15, 8)\n",
    "print(neigh)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# TT, TVT e nCV\n",
    "\n",
    "#def tt(X, y, test_size):\n",
    "\n",
    "def cross_validation(train_X, train_y, num_folds=10, k=1):\n",
    "    dataset = list()\n",
    "    dataset_split = list()\n",
    "    val_acc = list()\n",
    "    \n",
    "    for i in range(len(train_X)):\n",
    "        data = np.append(train_X[i],train_y[i])\n",
    "        dataset.append(data)\n",
    "    \n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / num_folds)\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "        \n",
    "    for folds in dataset_split:\n",
    "        train_set= folds\n",
    "        train_set = np.array(train_set)\n",
    "        test_set = list()\n",
    "        for row in folds:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        test_set = np.array(test_set)\n",
    "        train_x = train_set[:, :-1]\n",
    "        train_y = train_set[:,-1]\n",
    "        test_x = test_set[:, :-1]\n",
    "        predicted = predict(train_x,train_y, test_x, k)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = compute_accuracy(actual, predicted)\n",
    "        val_acc.append(accuracy)\n",
    "        \n",
    "    val_acc_var = statistics.variance(val_acc)\n",
    "    vall_acc = sum(val_acc)/len(val_acc)\n",
    "\n",
    "    return vall_acc, val_acc_var\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Analise os resultados em termos de **bias-variance** e **underfitting-overfitting**\n",
    "\n",
    "Thinking about a dartboard:\n",
    "- Low bias,  low variance  : population is **close** to center; individuals are **close** to each other\n",
    "- Low bias,  high variance : population is **close** to center; individuals are **far** from each other\n",
    "- High bias, low variance  : population is **far** from the center; individuals are **close** to each other\n",
    "- High bias, high variance : population is **far** from the center; individuals are **far** from each other\n",
    "    \n",
    "\n",
    "**Underfitting**: model does not predict the data very well <br>\n",
    "**Overfitting**: model predicts the data too well <br>\n",
    "\n",
    "Our model:\n",
    "- Low bias: The predicted results are almost always right\n",
    "- Low variance: The results are close to each other, as almost all of them are in the same position\n",
    "- Overfitted: there are few classes and features for classification. Classification is very easy.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Repita a experiência anterior usando o algoritmo ReliefF para obter o ranking de features e seleccionar o modelo ideal (em termos de parâmetros e features a utilizar)\n",
    "\n",
    "### 2.2.1. Utilize a métrica F1-score como critério óptimo para a escolha de features e parâmetros a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TT 70-30\n",
    "input_data, target_data = iris_input, iris_target\n",
    "\n",
    "n_neig = 1\n",
    "n_feat = 2\n",
    "fs = ReliefF(n_neighbors=n_neig, n_features_to_keep=n_feat)\n",
    "input_data = fs.fit_transform(input_data, target_data)\n",
    "\n",
    "# print(\"--------------\")\n",
    "# print(\"(No. of tuples, No. of Columns before ReliefF) : \"+str(input_data.shape)+\n",
    "      # \"\\n(No. of tuples , No. of Columns after ReliefF) : \"+str(input_train.shape))\n",
    "\n",
    "print(fs.top_features)\n",
    "print(fs.feature_scores)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Visualize o “gráfico do cotovelo” relativo ao desempenho do modelo (no conjunto de validação) à medida que se vão adicionando features\n",
    "\n",
    "Distortion increases as we consider more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, target_data = iris_input, iris_target\n",
    "\n",
    "n_neig = 1\n",
    "n_feat = 2\n",
    "\n",
    "fs = ReliefF(n_neighbors=n_neig, n_features_to_keep=n_feat)\n",
    "test_data = fs.fit_transform(input_data, target_data)\n",
    "\n",
    "top_features = fs.top_features\n",
    "print(top_features)\n",
    "total_features = len(input_data[0])\n",
    "print(total_features)\n",
    "for i in range(1, total_features+1):\n",
    "    print(\"Feature \", top_features[i-1])\n",
    "    # create indexes\n",
    "    indexes = top_features[:i]\n",
    "    \n",
    "    input_data = iris_input[:, indexes]\n",
    "    \n",
    "    df = pd.DataFrame(input_data)\n",
    "    df['target'] = target_data\n",
    "    \n",
    "    distortions = []\n",
    "    K = range(1,10)\n",
    "    for k in K:\n",
    "        kmeanModel = KMeans(n_clusters=k)\n",
    "        kmeanModel.fit(df)\n",
    "        distortions.append(kmeanModel.inertia_)\n",
    "    plt.figure()\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method showing the optimal k')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#df = pd.DataFrame(iris_data.data, columns = iris_data.feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "## TVT 40-30-30\n",
    "print(\"----- TVT 40-30-30 -----\")\n",
    "\n",
    "target_classes = iris_classes\n",
    "input_train, input_val, input_test, target_train, target_val, target_test = tvt(iris_input, iris_target, 57.1428, 30) # 3 simples -> 70-100; 40-x; x = 40*100/70 = 57.1428 \n",
    "\n",
    "k_neigh = [1, 3, 5, 7, 9, 11, 13, 15]\n",
    "\n",
    "mean_recall = 0.00\n",
    "mean_precision = 0.00\n",
    "mean_f1 = 0.00\n",
    "\n",
    "print(\"-- 1st iteration (with val set)\")\n",
    "for k in k_neigh:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(input_train, target_train)\n",
    "    target_val_pred = knn.predict(input_val)\n",
    "\n",
    "    recall, precision, f1 = exact_methods(target_val, target_val_pred, 'weighted')\n",
    "    #cm = plot_cm(target_val, target_val_pred, target_classes)\n",
    "\n",
    "    mean_recall += recall\n",
    "    mean_precision += precision\n",
    "    mean_f1 += f1\n",
    "\n",
    "print(\"MEAN RECALL:\", mean_recall/len(k_neigh))\n",
    "print(\"MEAN PRECISION:\", mean_precision/len(k_neigh))\n",
    "print(\"MEAN F1 SCORE:\", mean_f1/len(k_neigh), \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "neighbours = 5\n",
    "x_relief = relief(X, y, neighbours, 10) # TODO qual o numero de features que se quer\n",
    "# TODO tem que se alterar o algoritmo de relief?\n",
    "print(x_relief.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_relief, y, test_size=0.3, random_state=1)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_train, y_train) #train\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "f1_score(y_pred, y_test, average=None)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "error_rate = []# Will take some time\n",
    "for i in range(1, X_train.shape[1]+1):\n",
    "    print(i)\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,X_train.shape[1]+1),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    " markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Análise ao *dataset* original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = []\n",
    "\n",
    "for i in range (0,15):\n",
    "    loc.append(\"FORTH_TRACE_DATASET-master/part\" + str(i) + \"/part\" + str(i) + \"dev2.csv\")\n",
    "\n",
    "array = []\n",
    "for file in loc:\n",
    "    df = pd.read_csv(file, sep=',', header=None)\n",
    "    array.append(df.to_numpy())\n",
    "\n",
    "array = np.concatenate(array)\n",
    "print(array.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# 4. Repita o ponto 2 no dataset de actividades humanos utilizado neste trabalho, usando data splitting **TVT** (apenas TVT; CV não é para fazer), uma **rede neuronal feedforward** (MLP) com **3 camadas**, **número variável de neurónios na camada escondida**, função de **activação logística** em todos os neurónios, **batch learning** e as features seleccionadas nas alíneas anteriores:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Com velocidade de aprendizagem fixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, target_data = iris_input, iris_target\n",
    "\n",
    "input_train, input_val, input_test, target_train, target_val, target_test = tvt(input_data, target_data, 30, 30)\n",
    "\n",
    "feedforward = MLPClassifier(learning_rate='adaptive', activation='logistic', batch_size='lbfgs', solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, ), random_state=1)\n",
    "\n",
    "feedforward.fit(input_train, target_train)\n",
    "\n",
    "print(target_test)\n",
    "target_pred = feedforward.predict(input_test)\n",
    "print(target_pred)\n",
    "\n",
    "recall, precision, f1 = exact_methods(target_test, target_pred, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9e7db499751c2962e04d58d0fc15e80a4a2bcb2fa278d126f3ba3a8ebfee268"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

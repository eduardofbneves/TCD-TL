{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte B. \n",
    "### Elaboração de um conjunto de scripts e funções em Python, NumPy, SciPy e Scikit-learn para realizar as tarefas de Aprendizagem Computacional e Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# importar livraria que contem as funcoes das features\n",
    "import featuresFunctions\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# ex 4\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# 0. Utility functions\n",
    "### 0.1. Loading human activities dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0000e+00, 3.0317e+00, 9.0450e+00, ..., 6.8849e-01, 1.2654e+00,\n",
       "        6.3629e+01],\n",
       "       [2.0000e+00, 2.9591e+00, 9.0436e+00, ..., 6.8849e-01, 1.2741e+00,\n",
       "        8.3160e+01],\n",
       "       [2.0000e+00, 2.9465e+00, 9.0545e+00, ..., 6.7857e-01, 1.2675e+00,\n",
       "        1.0269e+02],\n",
       "       ...,\n",
       "       [2.0000e+00, 1.7283e+00, 9.5641e+00, ..., 9.2857e-01, 1.3531e+00,\n",
       "        1.0420e+06],\n",
       "       [2.0000e+00, 1.7285e+00, 9.5646e+00, ..., 9.0675e-01, 1.3377e+00,\n",
       "        1.0420e+06],\n",
       "       [2.0000e+00, 1.6802e+00, 9.4907e+00, ..., 8.6310e-01, 1.3180e+00,\n",
       "        1.0420e+06]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_human_activities_dataset():\n",
    "    loc = []\n",
    "\n",
    "    for i in range (0,15):\n",
    "        loc.append(\"FORTH_TRACE_DATASET-master/part\" + str(i) + \"/part\" + str(i) + \"dev2.csv\")\n",
    "\n",
    "    array = []\n",
    "    for file in loc:\n",
    "        df = pd.read_csv(file, sep=',', header=None)\n",
    "        array.append(df.to_numpy())\n",
    "\n",
    "    array = np.concatenate(array)\n",
    "    #print(array.shape)\n",
    "\n",
    "    return array\n",
    "\n",
    "human_activities_data = load_human_activities_dataset()\n",
    "human_activities_input = human_activities_data[:, :11]\n",
    "human_activities_target = human_activities_data[:, -1]\n",
    "\n",
    "dummy = human_activities_data[:6]\n",
    "dummy_input = dummy[:, :11]\n",
    "dummy_target = dummy[:, -1]\n",
    "\n",
    "human_activities_input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Loading Iris dataset\n",
    "\n",
    "150 instances <br>\n",
    "4 atributes:\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "\n",
    "3 classes:\n",
    "- Iris Setosa\n",
    "- Iris Versicolour\n",
    "- Iris Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                  5.1               3.5                1.4               0.2   \n",
      "1                  4.9               3.0                1.4               0.2   \n",
      "2                  4.7               3.2                1.3               0.2   \n",
      "3                  4.6               3.1                1.5               0.2   \n",
      "4                  5.0               3.6                1.4               0.2   \n",
      "..                 ...               ...                ...               ...   \n",
      "145                6.7               3.0                5.2               2.3   \n",
      "146                6.3               2.5                5.0               1.9   \n",
      "147                6.5               3.0                5.2               2.0   \n",
      "148                6.2               3.4                5.4               2.3   \n",
      "149                5.9               3.0                5.1               1.8   \n",
      "\n",
      "     target  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "..      ...  \n",
      "145       2  \n",
      "146       2  \n",
      "147       2  \n",
      "148       2  \n",
      "149       2  \n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "\n",
    "df = pd.DataFrame(iris_data.data, columns = iris_data.feature_names)\n",
    "df['target'] = iris_data.target\n",
    "print(df)\n",
    "iris_df = df\n",
    "iris_input = iris_data.data\n",
    "iris_target = iris_data.target\n",
    "\n",
    "iris_input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# 1. Data splitting e métricas de exactidão em machine learning\n",
    "### 1.1. Usando o scikit-learn, desenvolva um conjunto de funções para **data splitting** usando dois cenários:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Train-Test (TT) e Train-Validation-Test data split\n",
    "\n",
    "Divides x (input) and y (target) in: <br>\n",
    "- x_train = x * test_perc   \n",
    "- x_val = x * val_perc         \n",
    "- x_test = x - x_train - x_val\n",
    "- y_train = y * test_perc     \n",
    "- y_val = y * val_perc\n",
    "- y_test = y - y_train - y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tt(x, y, test_perc):\n",
    "    test_size = test_perc * 0.01\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=1)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def tvt(x, y, val_perc, test_perc):\n",
    "    test_size = test_perc * 0.01\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=1)\n",
    "\n",
    "    # 80 train, 20 test -> 60 train, 20 validation, 20 test\n",
    "    val_size = test_perc * 0.01\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_size, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. K-fold data split\n",
    "\n",
    "K-folds cross-validator. Returns k indexes of training and validating (min - 2. max - samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_indexes(data_input, splits):\n",
    "    kf = KFold(n_splits=splits)\n",
    "\n",
    "    x_indexes_train=[]\n",
    "    x_indexes_test=[]\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(data_input)):\n",
    "        x_indexes_train.append(train_index)\n",
    "        x_indexes_test.append(test_index)\n",
    "\n",
    "        print(train_index, test_index)\n",
    "\n",
    "    return x_indexes_train, x_indexes_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5] [0 1]\n",
      "[0 1 3 4 5] [2]\n",
      "[0 1 2 4 5] [3]\n",
      "[0 1 2 3 5] [4]\n",
      "[0 1 2 3 4] [5]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "### TEST CODE\n",
    "\n",
    "# TVT\n",
    "#dummy_input_train, dummy_input_val, dummy_input_test, dummy_target_train, dummy_target_val, dummy_target_test = tvt(human_activities_input, human_activities_target, 20, 20)\n",
    "\n",
    "#print(human_activities_input.shape, human_activities_target.shape, dummy_input_train.shape, dummy_input_val.shape, dummy_input_test.shape, dummy_target_train.shape, dummy_target_val.shape, dummy_target_test.shape)\n",
    "\n",
    "# K-FOLD\n",
    "dummy_train_indexes, dummy_test_indexes = k_fold_indexes(dummy_target, 5)\n",
    "print(\"---\")\n",
    "\n",
    "def k_fold_arrays(data_input, splits):\n",
    "    data_input_train = []\n",
    "    data_input_test = []\n",
    "\n",
    "    train_indexes, test_indexes = k_fold_indexes(data_input, splits)\n",
    "\n",
    "    print(\"---\")\n",
    "    for i in train_indexes:\n",
    "        #print(i)\n",
    "        \n",
    "        data_input_train.append(data_input[i])\n",
    "        data_target_train.append(data_target[i])\n",
    "\n",
    "        #print(data_input_train, \"--\\n\", data_target_train)\n",
    "\n",
    "    print(\"---\")\n",
    "    for j in test_indexes:\n",
    "        print(j)\n",
    "        \n",
    "        data_input_test.append(data_input[j])\n",
    "        data_target_test.append(data_target[j])\n",
    "\n",
    "        print( \"--\\n\", data_target_test)\n",
    "\n",
    "\n",
    "\n",
    "#dummy_input\n",
    "\n",
    "#k_fold_arrays(dummy_input, dummy_target, 5)\n",
    "\n",
    "#dummy_input_by_indexes = dummy_input[dummy_train_indexes[0]]\n",
    "#dummy_input_by_indexes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#recall, precision, f1 = exact_methods(dummy_real, dummy_pred, 'macro')\n",
    "#print(recall, precision, f1) # com average = 'micro' os resultados sao iguais\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Usando o scikit-learn, desenvolva um conjunto de funções para cálculo de **métricas de exactidação**, nomeadamente as seguintes:\n",
    "\n",
    "### 1.1.2. Matriz de confusão\n",
    "### 1.2.2. Recall\n",
    "### 1.2.3. Precision\n",
    "### 1.2.4. F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "#from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "def confusion(knn, y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    matrix = plot_confusion_matrix(knn, x_test, y_test, cmap=plt.cm.Blues)\n",
    "    matrix.ax_.set_title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.gcf().axes[0]\n",
    "    plt.gcf().axes[1]\n",
    "    return cm, plt\n",
    "\n",
    "def exact_methods(y_true, y_pred, av):\n",
    "    '''\n",
    "    av = micro\n",
    "    av = macro\n",
    "    av = binary\n",
    "    av = samples\n",
    "    av = weighted\n",
    "    '''\n",
    "    recall = recall_score(y_true, y_pred, average=av) # average=?\n",
    "    precision = precision_score(y_true, y_pred, average=av)\n",
    "    f1 = f1_score(y_true, y_pred, average=av)\n",
    "\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"F1 score: \", f1)\n",
    "\n",
    "    return recall, precision, f1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experiências iniciais com um classificador simples, i.e., k-Nearest Neighbours (kNN)\n",
    "\n",
    "### 2.1. Usando:\n",
    "- as funções anteriores\n",
    "- o algoritmo kNN\n",
    "- o dataset Iris\n",
    "\n",
    "avalie a capacidade de previsão do algoritmo neste dataset, com **k = 1** e restantes parâmetros por omissão (e.g., métrica de distância, etc.), e usando todo o conjunto de features, nos seguintes cenários:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Train-only, TT 70-30 e 10x10-fold cross-validation (10CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0 1 2 2 0 2 2 1]\n",
      "[0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0 1 2 2 0 1 2 1]\n",
      "Recall:  [1.         1.         0.92307692]\n",
      "Precision:  [1.         0.94736842 1.        ]\n",
      "F1 score:  [1.         0.97297297 0.96      ]\n"
     ]
    }
   ],
   "source": [
    "input_train, input_test, target_train, target_test = tt(iris_input, iris_target, 30)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(input_train, target_train)\n",
    "target_pred = knn.predict(input_test)\n",
    "\n",
    "print(target_test)\n",
    "print(target_pred)\n",
    "\n",
    "recall, precision, f1 = exact_methods(target_test, target_pred, None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.93333333 1.         1.         0.86666667 0.93333333\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "cv_scores mean:0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = tt(X, y, 0.5)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train) #train\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "acc = knn.score(X_test, y_test) #test\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn_cv = KNeighborsClassifier(n_neighbors=6)\n",
    "#train model with cv of 5 \n",
    "cv_scores = cross_val_score(knn_cv, X, y, cv=10)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print('cv_scores mean:{}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = np.linspace(1, 15, 8)\n",
    "\n",
    "for n in neighbours:\n",
    "    knn = KNeighborsClassifier(n_neighbors=int(n))\n",
    "    knn.fit(X, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = np.linspace(1, 15, 8)\n",
    "print(neigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TT, TVT e nCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def tt(X, y, test_size):\n",
    "\n",
    "def cross_validation(train_X, train_y, num_folds=10, k=1):\n",
    "    dataset = list()\n",
    "    dataset_split = list()\n",
    "    val_acc = list()\n",
    "    \n",
    "    for i in range(len(train_X)):\n",
    "        data = np.append(train_X[i],train_y[i])\n",
    "        dataset.append(data)\n",
    "    \n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / num_folds)\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "        \n",
    "    for folds in dataset_split:\n",
    "        train_set= folds\n",
    "        train_set = np.array(train_set)\n",
    "        test_set = list()\n",
    "        for row in folds:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        test_set = np.array(test_set)\n",
    "        train_x = train_set[:, :-1]\n",
    "        train_y = train_set[:,-1]\n",
    "        test_x = test_set[:, :-1]\n",
    "        predicted = predict(train_x,train_y, test_x, k)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = compute_accuracy(actual, predicted)\n",
    "        val_acc.append(accuracy)\n",
    "        \n",
    "    val_acc_var = statistics.variance(val_acc)\n",
    "    vall_acc = sum(val_acc)/len(val_acc)\n",
    "\n",
    "    return vall_acc, val_acc_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobre overfitting\n",
    "\n",
    "Overfitting refers to an unwanted behavior of a machine learning algorithm used for predictive modeling.\n",
    "\n",
    "It is the case where model performance on the training dataset is improved at the cost of worse performance on data not seen during training, such as a holdout test dataset or new data.\n",
    "\n",
    "We can identify if a machine learning model has overfit by first evaluating the model on the training dataset and then evaluating the same model on a holdout test dataset.\n",
    "\n",
    "If the performance of the model on the training dataset is significantly better than the performance on the test dataset, then the model may have overfit the training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usar as features dadas pelo Relief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = 5\n",
    "x_relief = relief(X, y, neighbours, 10) # TODO qual o numero de features que se quer\n",
    "# TODO tem que se alterar o algoritmo de relief?\n",
    "print(x_relief.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_relief, y, test_size=0.3, random_state=1)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_train, y_train) #train\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "f1_score(y_pred, y_test, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = []# Will take some time\n",
    "for i in range(1, X_train.shape[1]+1):\n",
    "    print(i)\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,X_train.shape[1]+1),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    " markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Análise ao *dataset* original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = []\n",
    "\n",
    "for i in range (0,15):\n",
    "    loc.append(\"FORTH_TRACE_DATASET-master/part\" + str(i) + \"/part\" + str(i) + \"dev2.csv\")\n",
    "\n",
    "array = []\n",
    "for file in loc:\n",
    "    df = pd.read_csv(file, sep=',', header=None)\n",
    "    array.append(df.to_numpy())\n",
    "\n",
    "array = np.concatenate(array)\n",
    "print(array.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# 4. Repita o ponto 2 no dataset de actividades humanos utilizado neste trabalho, usando data splitting **TVT** (apenas TVT; CV não é para fazer), uma **rede neuronal feedforward** (MLP) com **3 camadas**, **número variável de neurónios na camada escondida**, função de **activação logística** em todos os neurónios, **batch learning** e as features seleccionadas nas alíneas anteriores:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Com velocidade de aprendizagem fixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0 1 2 2 0 2 2 1]\n",
      "[0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 2 0 0 1 2 2 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0 1 2 2 0 1 2 1]\n",
      "Recall:  [1.         0.83333333 0.92307692]\n",
      "Precision:  [1.     0.9375 0.8   ]\n",
      "F1 score:  [1.         0.88235294 0.85714286]\n"
     ]
    }
   ],
   "source": [
    "input_data, target_data = iris_input, iris_target\n",
    "\n",
    "input_train, input_val, input_test, target_train, target_val, target_test = tvt(input_data, target_data, 30, 30)\n",
    "\n",
    "feedforward = MLPClassifier(learning_rate='adaptive', activation='logistic', batch_size='lbfgs', solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, ), random_state=1)\n",
    "\n",
    "feedforward.fit(input_train, target_train)\n",
    "\n",
    "print(target_test)\n",
    "target_pred = feedforward.predict(input_test)\n",
    "print(target_pred)\n",
    "\n",
    "recall, precision, f1 = exact_methods(target_test, target_pred, None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9e7db499751c2962e04d58d0fc15e80a4a2bcb2fa278d126f3ba3a8ebfee268"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
